{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def plot_something(df, x_col, y_col, title=None, x_col_name=None, y_col_name=None, filename=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(df[x_col], df[y_col], marker='o', label=f'{y_col} vs {x_col}')\n",
    "    \n",
    "    if x_col_name:\n",
    "        plt.xlabel(x_col_name)\n",
    "    else:\n",
    "        plt.xlabel(x_col)\n",
    "        \n",
    "    if y_col_name:\n",
    "        plt.ylabel(y_col_name)\n",
    "    else:\n",
    "        plt.ylabel(y_col)\n",
    "        \n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(f'{y_col} vs {x_col}')\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    if labels:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_csv_exp1(df, dirpath=None, title_postfix=None):\n",
    "   \n",
    "    serial_time = df[df['Processes'] == 1]['ExecutionTime'].values[0]\n",
    "\n",
    "    # Obliczenie przyspieszenia\n",
    "    df['Speedup'] = serial_time / df['ExecutionTime']\n",
    "    \n",
    "    # Obliczenie efektywności\n",
    "    df['Efficiency'] = df['Speedup'] / df['Processes']\n",
    "    \n",
    "    # Obliczenie serial fraction\n",
    "    df['SerialFraction'] = (1 / df['Speedup'] - 1 / df['Processes']) / (1 - 1 / df['Processes'])\n",
    "\n",
    "    # Wykres przyspieszenia\n",
    "    plot_something(\n",
    "        df, \n",
    "        \"Processes\", \n",
    "        \"Speedup\",\n",
    "        title=f\"Speedup vs Processes {title_postfix}\" if title_postfix else None,\n",
    "        filename=Path(dirpath)/\"speedup.png\" if dirpath else None)\n",
    "\n",
    "    # Wykres efektywności\n",
    "    plot_something(\n",
    "        df, \n",
    "        \"Processes\", \n",
    "        \"Efficiency\", \n",
    "        title=f\"Efficiency vs Processes {title_postfix}\" if title_postfix else None,\n",
    "        filename=Path(dirpath)/\"efficiency.png\" if dirpath else None)\n",
    "\n",
    "    # Wykres serial fraction\n",
    "    plot_something(\n",
    "        df, \n",
    "        \"Processes\", \n",
    "        \"SerialFraction\", \n",
    "        y_col_name=\"Serial Fraction\", \n",
    "        title=f\"Serial Fraction vs Processes {title_postfix}\" if title_postfix else \"Serial Fraction vs Processes\",\n",
    "        filename=Path(dirpath)/\"serial_fraction.png\" if dirpath else None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_dir = Path(\"./contact_order\")\n",
    "\n",
    "csv_filename = co_dir / \"logs.csv\"\n",
    "\n",
    "df_co_logs = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_something(\n",
    "    df=df_co_logs, \n",
    "    x_col=\"end_idx\", \n",
    "    y_col=\"checkpoint_duration\",\n",
    "    x_col_name=\"Numer of processed files\",\n",
    "    y_col_name=\"Checkpoint duration time [s]\",\n",
    "    filename=co_dir / \"exec_time_vs_checkpoints.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"./contact_order/contact_order_results.csv\"\n",
    "\n",
    "df_co_results = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_percent = round(len(df_co_results[df_co_results[\"contact_order\"].isna()])/len(df_co_results)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error files: 6.65%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Error files: {error_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error storage_client: 6.1%\n",
      "Error gsutil: 0.0%\n",
      "Error gcloud_storage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def str2sec(time_str):\n",
    "    h, m, s = map(int, time_str.split(\":\"))\n",
    "    total_seconds = h * 3600 + m * 60 + s\n",
    "    return total_seconds\n",
    "\n",
    "source_dir = \"/home/piotrek/Documents/personal/STUDIA_DS/semestr_2/LSC/projekt/download_tests\"\n",
    "dst_dir = \"/home/piotrek/Documents/personal/STUDIA_DS/semestr_2/LSC/projekt/download_tests_results\"\n",
    "\n",
    "\n",
    "for dir_type in Path(source_dir).iterdir():\n",
    "    if dir_type.is_dir():\n",
    "        \n",
    "        type_name = Path(dir_type).name\n",
    "        results_type = pd.DataFrame([])\n",
    "        \n",
    "        error_results = []\n",
    "        \n",
    "        for dir_type_worker in dir_type.iterdir():\n",
    "            if dir_type_worker.is_dir():\n",
    "                match = re.search(r\".*_(.*?)_.*\", dir_type_worker.name)\n",
    "                num_workers = match.group(1)\n",
    "                df_type_worker = pd.read_csv(dir_type_worker / \"download_progress.csv\")\n",
    "                df_type_worker[\"rel_time_sec\"] = df_type_worker[\"rel_time\"].apply(str2sec)\n",
    "                total_exec_time = int(df_type_worker[\"rel_time_sec\"].iloc[-1])\n",
    "                df_type_worker[\"rel_time_sec\"] = df_type_worker[\"rel_time_sec\"].diff(periods=1)\n",
    "                df_type_worker.dropna(inplace=True)\n",
    "                \n",
    "                df_type_worker_err = pd.read_csv(dir_type_worker / \"download_error.txt\")\n",
    "                num_error = len(df_type_worker_err)\n",
    "                error_results.append(num_error)\n",
    "                \n",
    "                save_dir_type_worker = Path(dst_dir) / os.path.relpath(dir_type_worker, source_dir)\n",
    "                save_dir_type_worker.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                plot_something(\n",
    "                    df=df_type_worker, \n",
    "                    x_col=\"num_file\", \n",
    "                    y_col=\"rel_time_sec\", \n",
    "                    title=f\"{num_workers} workers - {type_name}\",\n",
    "                    filename=save_dir_type_worker/\"relt_vs_numf.png\"\n",
    "                )\n",
    "                \n",
    "                results_type = pd.concat(\n",
    "                    [\n",
    "                        results_type,\n",
    "                        pd.DataFrame.from_dict(\n",
    "                            {\n",
    "                                \"Processes\": [int(num_workers)],\n",
    "                                \"ExecutionTime\": [total_exec_time]\n",
    "                            }\n",
    "                        )\n",
    "                        ]\n",
    "                    )\n",
    "        \n",
    "        total_files = int(df_type_worker[\"num_file\"].iloc[-1]) + 1\n",
    "        mean_type_error = round(sum(error_results)/(len(error_results)*total_files) * 100, 2)\n",
    "        print(f\"Error {type_name}: {mean_type_error}%\")\n",
    "\n",
    "            \n",
    "        save_dir_type = Path(dst_dir) / os.path.relpath(dir_type, source_dir)\n",
    "        save_dir_type.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results_type = results_type.sort_values(by=\"Processes\")\n",
    "        \n",
    "        plot_something(\n",
    "                df=results_type, \n",
    "                x_col=\"Processes\", \n",
    "                y_col=\"ExecutionTime\", \n",
    "                title=f\"Total download time vs number of processes - {type_name}\",\n",
    "                x_col_name=\"Processes [s]\",\n",
    "                filename=save_dir_type/\"exect_vs_proc.png\"\n",
    "            )    \n",
    "        plot_csv_exp1(results_type, dirpath=save_dir_type, title_postfix=f\" - {type_name}\")\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wnioski:\n",
    "\n",
    "- limit plików na `$SCRATCH` - 1 mln\n",
    "- ten zbiór wymaga zalogowanego użytkownika w google\n",
    "- w związku z czym był problem np z wgetem (nie chciało mi się walczyć z tą autoryzacją)\n",
    "- użycie każdej metody wymagało utworzenia projektu w Google Cloud oraz pozyskanie klucza API\n",
    "- w związku z limitem plików zaimlementowaliśmy rozwiązanie batchowe - pobieranie, przetwarzanie, usuwanie plików w batchu\n",
    "- `gsutil` oraz `gcloud storage` nie są dostępne w infrastrukturze plgrid, w związku z czym testy przeprowadzilismy lokalnie\n",
    "- contact order liczymy niezależnie dla każdego wątku, w związku z czym zakładamy, że skaluje się liniowo, więc badaliśmy jedynie ograniczenia z tytułu równoległego pobierania plików\n",
    "- istotne jest to, że `gsutil` oraz `gcloud storage` obciążały dekompresją system, stąd trwało to najprawdopodobniej dłużej. Szukałem jednak info jak robi to libka pythonowa i też wygląda na to, że stosuje kompresję i dekompresje przy przesyłaniu, ale nie jest to obciążające. Być może źle używałem tych dwóch libek i należało podać plik z manifestem, ale jedyne rozwiAzania jakie widzialem to np `gsutil -m cp -R gs://your-bucket .`, gdzie my nie chcemy calego bucketa xD. ewentualnie można było skleic jakiegos super długiego regexa, ale nie wiem, czy to dobry pomysł tworzyć tak długie zapytania xD\n",
    "- jak coś, to wszystkie procesy były odpalane na procesorze 6C 12T, zmieniałem wszystkie parametry w `ThreadPool` z libki `concurrent`, mogłem wrzucic tak duzo procesów, ponieważ biblioteka robi kolejke dla tasków, a pobieranie trochę trwa i jest mało procesorożerne (chyba, że `gsutil` lub `google storage`, gdize faktycnzie ograniczenie ze strony procesora weszło)\n",
    "- w przypadku libki pythonowej występowały błędy związane z pobieraniem (około ~6 plików), natomiast w przypadku plików pobieranych za pomocą `gsutil` oraz `google storage` - wszystkie pliki pobiarły się poprawnie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_lsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
